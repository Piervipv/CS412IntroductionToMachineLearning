{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this notebook we load the dataset we considered in Preprocessing0.ipynb and perform a different preprocessing.\n",
    "The structure of the notebook is similar with Preprocessinc0.ipynb, the main difference is in how the text from the original dataset is parsed.<br> <br>\n",
    "In particular, in building the BagOfWord Model we filter the words in the vocabulary avoiding to match meaningless words (such as words without any alphabetical character or composed only by puntaction characters).<br> <br>\n",
    "We also apply stemming with the PorterStemmer.<br> <br>\n",
    "\n",
    "#### In addition (w.r.t. Preprocessinc1.ipynb) here we perform also StopWord Removal.\n",
    " Finally in ModelsComparison2.ipynb we will compare different models (in the same way we did in ModelsCoparison0.ipynb) but on the different BagOfWord model and check if the classifiers benefit of the different preproccessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe management\n",
    "import pandas as pd             \n",
    "\n",
    "# numerical computation\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424368</td>\n",
       "      <td>532eafa4f3c40e97872d</td>\n",
       "      <td>Why am I not ticklish?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>487555</td>\n",
       "      <td>5f7adc919173a8ce382b</td>\n",
       "      <td>Why do people say homeopathy does not work whe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>813017</td>\n",
       "      <td>9f4c454107555556944f</td>\n",
       "      <td>What if someone merges a question you answered...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>956454</td>\n",
       "      <td>bb67c4798b448b68d48e</td>\n",
       "      <td>What causes variations in DLC?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>935420</td>\n",
       "      <td>b750325197b2ba03ec35</td>\n",
       "      <td>Can indefinite integrals have different soluti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   qid  \\\n",
       "0      424368  532eafa4f3c40e97872d   \n",
       "1      487555  5f7adc919173a8ce382b   \n",
       "2      813017  9f4c454107555556944f   \n",
       "3      956454  bb67c4798b448b68d48e   \n",
       "4      935420  b750325197b2ba03ec35   \n",
       "\n",
       "                                       question_text  target  \n",
       "0                             Why am I not ticklish?       0  \n",
       "1  Why do people say homeopathy does not work whe...       0  \n",
       "2  What if someone merges a question you answered...       0  \n",
       "3                     What causes variations in DLC?       0  \n",
       "4  Can indefinite integrals have different soluti...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./subdataset.csv')\n",
    "dataset.head() #show the first n instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows: 10000\n",
      "number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "#Let's check the dimension of the dataset\n",
    "rows = dataset.shape[0]\n",
    "print(\"number of rows: \" + str(rows))\n",
    "columns = dataset.shape[1]\n",
    "print(\"number of columns: \" + str(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.21% of instancies are positive\n",
      "93.79% of instancies are negative\n"
     ]
    }
   ],
   "source": [
    "#let's check the percentange of positive and negative examples\n",
    "positive = 0\n",
    "for row in dataset.itertuples():\n",
    "    positive += row.target\n",
    "\n",
    "print(str(positive*100/rows) + \"% of instancies are positive\")\n",
    "print(str(100-positive*100/rows) + \"% of instancies are negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class document:\n",
    "    def __init__(self,words, target):\n",
    "        self.words=words #dictionary of contained words\n",
    "        self.target=target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to create a database where the rows are vectors with #columns = size of vocabulary\n",
    "and for each row set the corresponding columns to 1 if the question contains that word, 0 otherwise.\n",
    "\n",
    "i.e.\n",
    "V = {cat, dog, mouse}\n",
    "\n",
    "q1: id = 1234; text = {cat, mouse}; target = 0 .  \n",
    "\n",
    "--> corresponding row: \n",
    "\n",
    "| 1234 (id) | 1 (cat) | 0 (dog) | 1 (mouse) | 0 (target) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 2 functions will take care of the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "### relative path of the file\n",
    "stopwords_path =\"./stopwords.txt\"  \n",
    "stopwords = open(stopwords_path).read()\n",
    "stopwords_list=stopwords.split()\n",
    "for stop in stopwords_list:\n",
    "    temp = stemmer.stem(stop)\n",
    "    if not (temp in stopwords_list):\n",
    "    \n",
    "       stopwords_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punt_and_number(string):\n",
    "    remove_list=['0','1','2','3','4','5','6','7','8','9','?','.' ]\n",
    "    #remove_list=[]\n",
    "    string2=''\n",
    "    for i in range(0,len(string)):\n",
    "        if not(string[i] in remove_list):\n",
    "            string2=string2 +string[i]\n",
    "    return string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def parse2(string):\n",
    "   # re.findall\n",
    "    tokens=[]\n",
    "    tokenizer=RegexpTokenizer('([a-z]*[-]?[a-z]+[-]?[a-z]*|[0-9]+[-]?[a-z]+|[a-z]+[-]?[0-9]+)') #matches only alphabetical characters\n",
    "    tokens=tokenizer.tokenize(string.lower())\n",
    "    \n",
    "    #split in single words the words such as \"a-class\"\n",
    "    tokens2 = []\n",
    "    for token in tokens:\n",
    "        tokens2 += token.split(\"-\")\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    processedWords=[]\n",
    "    \n",
    "    for word in tokens2:\n",
    "        token=remove_punt_and_number(word)\n",
    "        token=stemmer.stem(token);\n",
    "        #token=token.strip(\"-\")\n",
    "        alpha=re.findall('[a-z]', token)\n",
    "        if not(len(alpha)<3) and not(token in stopwords_list) : #condition to eliminate useless words and StopWords\n",
    "            processedWords.append(token) #stemming\n",
    "    return processedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done.\n",
      "10.0% done.\n",
      "20.0% done.\n",
      "30.0% done.\n",
      "40.0% done.\n",
      "50.0% done.\n",
      "60.0% done.\n",
      "70.0% done.\n",
      "80.0% done.\n",
      "90.0% done.\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#let's create a  basic bag of word model\n",
    "\n",
    "#we need an efficient dataStucture to build this model\n",
    "\n",
    "#let's create a dictionary of dictionary\n",
    "\n",
    "#questionDictionary: qid -> WordDictionary, target\n",
    "#WordDictionary: word_contained_in_given_question -> #number_of_occurencies\n",
    "\n",
    "#for each row:\n",
    "    #take the text and parse it (retrieve the list of contained words and update the vocaubolary of the collection)\n",
    "    #add an entry in the dictionary\n",
    "    \n",
    "questionDictionary = {}\n",
    "vocabulary = set()\n",
    "\n",
    "update = 0\n",
    "\n",
    "for row in dataset.itertuples():\n",
    "    wordDictionary = {}\n",
    "    if (update%1000 == 0):\n",
    "        print(str(round((update*100/rows),2)) + \"% done.\" )\n",
    "    update +=1\n",
    "    \n",
    "    ##THIS TIME LET'S USE A NEW PARSER:\n",
    "    words = parse2 (row.question_text)\n",
    "    vocabulary.update(words)\n",
    "    \n",
    "    #initialize the wordDictionary\n",
    "    for word in words:\n",
    "        wordDictionary[word] = 0\n",
    "        \n",
    "    #count the occurencies of each word\n",
    "    for word in words:\n",
    "        wordDictionary[word] += 1\n",
    "    \n",
    "    documentInstance = document (wordDictionary, row.target)\n",
    "    questionDictionary[row.qid]= documentInstance\n",
    "    \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the collection there are 10112 distinct words.\n",
      "first then words:\n",
      "['ban', 'armpit', 'lbgt', 'highschool', 'poor', 'inject', 'circadian', 'hold', 'pain', 'plaid']\n"
     ]
    }
   ],
   "source": [
    "#Let's check again the vocabulary\n",
    "print(\"in the collection there are \" + str(len(vocabulary)) + \" distinct words.\" )\n",
    "\n",
    "#print first 10 of them\n",
    "print(\"first then words:\")\n",
    "print(list(vocabulary)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'aadhaar', 'aadhar', 'aakash', 'aalto', 'aaron', 'aathar', 'abathroom', 'abbi', 'abbrevi', 'abdomin', 'abduct', 'abel', 'aberdeen', 'abid', 'abil', 'abiogenesi', 'ableism', 'abnorm', 'aboard', 'abolish', 'abomin', 'abort', 'abroad', 'abscess', 'absenc', 'absolut', 'absorb', 'abstract', 'abstrus', 'absurd', 'abudhabi', 'abus', 'academ', 'academi', 'academia', 'acaj', 'acc', 'acca', 'acccid']\n",
      "\n",
      "['bitsat', 'bitten', 'bitter', 'bittersweet', 'bizarr', 'bjmc', 'bjp', 'blabber', 'black', 'blackberri']\n"
     ]
    }
   ],
   "source": [
    "#Let's order alphabetically the vocabulary\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "#Let's explore the vocabulary\n",
    "print(vocabulary[0:40])\n",
    "print()\n",
    "print(vocabulary[990:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create the new DataSet\n",
    "\n",
    "#for every question:\n",
    "    #create a new row containing qID, bagOfWords, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import jit\n",
    "\n",
    "#@jit\n",
    "def extractRow(qid):\n",
    "    row = []\n",
    "    #row.append(qid)\n",
    "\n",
    "    #bagOfwords=[]\n",
    "    question = questionDictionary[qid]\n",
    "    questionWords = question.words\n",
    "    for word in vocabulary:\n",
    "        #if word in the text of the question:\n",
    "            #bagOfwords.append(1)\n",
    "        #else:\n",
    "            #bagOfwords.append(0)\n",
    "        ''''if word in document.words:\n",
    "            bagOfwords.append(1)\n",
    "        else:\n",
    "            bagOfwords.append(0)'''\n",
    "        \n",
    "        try:\n",
    "            #check if the word is contained in the question\n",
    "            row.append(questionWords[word])\n",
    "        except:\n",
    "            #if word is not in the dictionary cach the expeption\n",
    "            #and consider that word is not in that question\n",
    "            row.append(0)\n",
    "    #concatenate\n",
    "    #row += bagOfwords\n",
    "    row.append(question.target)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionIndexDictionary = {}\n",
    "index = 0\n",
    "for qid in questionDictionary.keys():\n",
    "    questionIndexDictionary[qid] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questionIndexDictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndexDictionary = {}\n",
    "index = 0\n",
    "for word in vocabulary:\n",
    "    wordIndexDictionary[word] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordIndexDictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsName = []\n",
    "columnsName += vocabulary\n",
    "columnsName.append('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([],columns=columnsName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done.\n",
      "10.0% done.\n",
      "20.0% done.\n",
      "30.0% done.\n",
      "40.0% done.\n",
      "50.0% done.\n",
      "60.0% done.\n",
      "70.0% done.\n",
      "80.0% done.\n",
      "90.0% done.\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# create new DataFrame\n",
    "index = 0\n",
    "for qid in questionDictionary.keys():\n",
    "#for qid in tqdm.tqdm(questionDictionary.keys()):\n",
    "    if (index%1000 == 0):\n",
    "        print(str(round((index*100/rows),2)) + \"% done.\" )\n",
    "    index +=1\n",
    "    \n",
    "    #qid = row.qid\n",
    "    #dataset.append(extractRow(qid))    \n",
    "    df.loc[questionIndexDictionary[qid]] =extractRow(qid)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>aadhar</th>\n",
       "      <th>aakash</th>\n",
       "      <th>aalto</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aathar</th>\n",
       "      <th>abathroom</th>\n",
       "      <th>abbi</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zoologist</th>\n",
       "      <th>zoroastrian</th>\n",
       "      <th>zstsn</th>\n",
       "      <th>zubeen</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zusak</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  aaa aadhaar aadhar aakash aalto aaron aathar abathroom abbi abbrevi  ...    \\\n",
       "0   0       0      0      0     0     0      0         0    0       0  ...     \n",
       "1   0       0      0      0     0     0      0         0    0       0  ...     \n",
       "2   0       0      0      0     0     0      0         0    0       0  ...     \n",
       "3   0       0      0      0     0     0      0         0    0       0  ...     \n",
       "4   0       0      0      0     0     0      0         0    0       0  ...     \n",
       "\n",
       "  zookeep zoolog zoologist zoroastrian zstsn zubeen zuckerberg zuni zusak  \\\n",
       "0       0      0         0           0     0      0          0    0     0   \n",
       "1       0      0         0           0     0      0          0    0     0   \n",
       "2       0      0         0           0     0      0          0    0     0   \n",
       "3       0      0         0           0     0      0          0    0     0   \n",
       "4       0      0         0           0     0      0          0    0     0   \n",
       "\n",
       "  TARGET  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 10113 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'./BagOfWordDataSet2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
